# -*- coding: utf-8 -*-
"""FAI_Mushroom.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b-6ID764DuNYneilx_B1JsOFMqrXYRlY

# Data Processing
"""

import pandas as pd

data=pd.read_csv("mushrooms.csv") # read the data from the file

data = pd.DataFrame(data) # convert the data into 2-dimensional data structure

print ("Original Dataframe : \n", data.head())

# check unique
unique_value=data.nunique()
print("\nThe unique value of dataset is \n",unique_value)

# Check missing values
missing_values = data.isnull().sum()
print("\nMissing values in each column:\n", missing_values)

# check duplication
duplicate_value = data.duplicated()
print ('\nDuplicated value : \n', duplicate_value)

# drop duplicate if havev
data = data.drop_duplicates()

x = data.drop('class', axis=1) # set x = all columns except column 'class'
y = data['class'] # set y (output) = class

print("Data after cleaning:")
print("Number of rows:", data.shape[0])
print("Number of columns:", data.shape[1])

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

label_encoder = LabelEncoder()

# Label Encoding to each feature
x_encoded = x.apply(label_encoder.fit_transform)
y_encoded = label_encoder.fit_transform(y)

# Check the encoded dataset
print("X_encoded is : \n",x_encoded.head())
print("Y_encoded is : \n",y_encoded[:5])

# Split the dataset into training and testing data sets
X_train, X_test, y_train, y_test = train_test_split(x_encoded, y_encoded, test_size=0.3, random_state=42)

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Process to find the optimal number of feature"""

import warnings
from sklearn.ensemble import RandomForestClassifier
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.metrics import make_scorer, f1_score
import matplotlib.pyplot as plt

# Ignore future warnings while training
warnings.simplefilter(action='ignore', category=FutureWarning)

# Initialize the Random Forest model
random_forest = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=3,
    max_features='sqrt',
    random_state=42
)

# Initialize F1 scorer metric
f1_scorer = make_scorer(f1_score, average='macro')

# Define a list to store the number of features and their corresponding f1-score
f1score_results = {}

# Loop over different numbers of of features
for k in range(1, X_train_scaled.shape[1] + 1):
    # Initialize SFS for the current number of features
    sfs = SFS(random_forest,
              k_features=k,
              forward=True,
              floating=False,
              scoring=f1_scorer,
              cv=5)  # 5-fold cross-validation for F1-score

    # Fit and train the feature with SFS
    sfs.fit(X_train_scaled, y_train)

    # Get the selected feature indices
    selected_feature_indices = list(sfs.k_feature_idx_)

    # Select only the chosen features for training and testing
    X_train_k_features = X_train_scaled[:, selected_feature_indices]
    X_test_k_features = X_test_scaled[:, selected_feature_indices]

    # Train the model with the selected features
    random_forest.fit(X_train_k_features, y_train)

    # Predict on the test set and calculate F1-score
    y_pred = random_forest.predict(X_test_k_features)
    f1_result = f1_score(y_test, y_pred, average='macro')  # Renamed variable

    # Store the F1-score for the current number of features
    f1score_results[k] = f1_result
    print(f"Number of features: {k}, F1 Score: {f1_result:.4f}")

# Find the optimal number of features
optimal_k = max(f1score_results, key=f1score_results.get)
print(f"Optimal number of features: {optimal_k} with F1 Score: {f1score_results[optimal_k]:.4f}")

# Plot the F1 Score results
plt.figure(figsize=(10, 6))
plt.plot(list(f1score_results.keys()), list(f1score_results.values()), marker='o')
plt.xlabel("Number of Features")
plt.ylabel("F1 Score")
plt.title("F1 Score vs. Number of Features")
plt.show()

"""# Feature Selection with the optimal number"""

import warnings

# Ignore the future warning while training the data
warnings.simplefilter(action='ignore', category=FutureWarning)


from sklearn.ensemble import RandomForestClassifier
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.metrics import make_scorer, f1_score

# Initialize the Random Forest model
random_forest = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=3,
    max_features='sqrt',
    random_state=42
)
# Gte F1-score as the evalution metrics
f1_scorer = make_scorer(f1_score, average='macro')

# Initialize SFS (Sequential Forward Selection)
sfs_rf = SFS(random_forest,
             k_features=5,  # select the top 10 features
             forward=True,
             floating=False,
             scoring=f1_scorer,
             cv=5)

# Perform feature selection
sfs_rf = sfs_rf.fit(X_train_scaled, y_train)

# Get selected feature indices
selected_features_rf = X_train.columns[list(sfs_rf.k_feature_idx_)]
print("Selected features for Random Forest:", selected_features_rf)

"""# Train with the selected feature"""

from sklearn.metrics import f1_score , precision_score, recall_score, accuracy_score

# Create new training and testing sets with selected features
X_train_selected = X_train_scaled[:, list(sfs_rf.k_feature_idx_)]
X_test_selected = X_test_scaled[:, list(sfs_rf.k_feature_idx_)]

# Train the model with the selected features
random_forest.fit(X_train_selected, y_train)

# Predictions on the test set and train set
random_forest_train_prediction = random_forest.predict(X_train_selected) # training set
random_forest_test_prediction = random_forest.predict(X_test_selected) # testing set

# Evaluate with F1-score for training set
train_f1_score = f1_score(y_train, random_forest_train_prediction, average='macro')
print(f"Training set: Random Forest F1-Score with selected features: {train_f1_score:.2f}\n")

# Evaluate with F1-score for testing set
test_f1_score = f1_score(y_test, random_forest_test_prediction, average='macro')
print(f"Testing set: Random Forest F1-Score with selected features: {test_f1_score:.2f}\n")

print("=" * 25)

# Evaluate with precision for training set and testing set
# train_precision = precision_score(y_train, random_forest_train_prediction, average='macro')
# print(f"Random Forest Precision Score with selected features: {train_precision:.2f}\n")
test_precision = precision_score(y_test, random_forest_test_prediction, average='macro')
print(f"Random Forest Precision Score with selected features: {test_precision:.2f}\n")

print("=" * 25)

# Evaluate with Recall for training set and testing set
# train_recall = recall_score(y_train, random_forest_train_prediction, average='macro')
# print(f"Random Forest Recall Score with selected features: {train_recall:.2f}")
test_recall = recall_score(y_test, random_forest_test_prediction, average='macro')
print(f"Random Forest Recall Score with selected features: {test_recall:.2f}")

print("=" * 25)

# Evaluate accuracy for testing set
test_accuracy = accuracy_score(y_test, random_forest_test_prediction)
print(f"Random Forest Testing Accuracy with selected features: {test_accuracy:.2f}\n")

print("=" * 25)

# Evaluate with precision for testing set
test_precision = precision_score(y_test, random_forest_test_prediction, average='macro')
print(f"Random Forest Precision Score with selected features: {test_precision:.2f}\n")

print("=" * 25)

# Evaluate with Recall for testing set
test_recall = recall_score(y_test, random_forest_test_prediction, average='macro')
print(f"Random Forest Recall Score with selected features: {test_recall:.2f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# confusion matrix
conf_matrix = confusion_matrix(y_test, random_forest_test_prediction)

# Display
print("Confusion Matrix:")
print(conf_matrix)
# Plot the confusion matrix
cfm = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=random_forest.classes_)
cfm.plot(cmap='Blues')

"""# Cross Validation Checking"""

from sklearn.model_selection import cross_val_score

# Use cross-validation to evaluate model
cv_scores = cross_val_score(random_forest, X_train_selected, y_train, cv=5)
print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean Cross-Validation Score: {cv_scores.mean():.2f}")

"""# Compare with KNN model"""

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.neighbors import KNeighborsClassifier
# Create new training and test sets with selected features
X_train_selected = X_train_scaled[:, list(sfs_rf.k_feature_idx_)]
X_test_selected = X_test_scaled[:, list(sfs_rf.k_feature_idx_)]


# KNN model with the selected features
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_selected, y_train)


# Make predictions (Inference)
knn_model_train_prediction = knn_model.predict(X_train_selected) # training set
knn_model_test_prediction = knn_model.predict(X_test_selected) # testing set

# F1-score for KNN Training Set
# train_f1_score = f1_score(y_train, knn_model_train_prediction, average='macro')
# print(f"Traing Set: KNN F1-Score with selected features: {train_f1_score:.2f}\n")

# F1-score for KNN Testing Set
test_f1_score = f1_score(y_test, knn_model_test_prediction, average='macro')
print(f"Testing Set: KNN F1-Score with selected features: {test_f1_score:.2f}\n")


# Evaluate accuracy for training set and testing set
# train_accuracy = accuracy_score(y_train, knn_model_train_prediction)
# print(f"KNN Training Accuracy with selected features: {train_accuracy:.2f}\n")
test_accuracy = accuracy_score(y_test, knn_model_test_prediction)
print(f"KNN Testing Accuracy with selected features: {test_accuracy:.2f}\n")

print("=" * 25)


# Evaluate with precision for training set and testing set
# train_precision = precision_score(y_train, knn_model_train_prediction, average='macro')
# print(f"KNN Precision Score with selected features: {train_precision:.2f}\n")
test_precision = precision_score(y_test, knn_model_test_prediction, average='macro')
print(f"KNN Precision Score with selected features: {test_precision:.2f}\n")

print("=" * 25)


# Evaluate with Recall for training set and testing set
# train_recall = recall_score(y_train, knn_model_train_prediction, average='macro')
# print(f"KNN  Recall Score with selected features: {train_recall:.2f}")
test_recall = recall_score(y_test, knn_model_test_prediction, average='macro')
print(f"KNN Recall Score with selected features: {test_recall:.2f}")



"""# Compare with Naive Bayes model"""

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.naive_bayes import GaussianNB

# Create new training and test sets with selected features
X_train_selected = X_train_scaled[:, list(sfs_rf.k_feature_idx_)]
X_test_selected = X_test_scaled[:, list(sfs_rf.k_feature_idx_)]

# Train the Naive Bayes model with the selected features
naive_bayes = GaussianNB()
naive_bayes.fit(X_train_selected, y_train)

# Make predictions on the training and testing set
naive_bayes_train_prediction = naive_bayes.predict(X_train_selected) # training set
naive_bayes_test_prediction = naive_bayes.predict(X_test_selected) # testing set

# F1-score for training set
train_f1_score = f1_score(y_train, naive_bayes_train_prediction, average='macro')
print(f"Traing Set: Naive Bayes F1-Score with selected features: {train_f1_score:.2f}\n")

# F1-score for testing set
test_f1_score = f1_score(y_test, naive_bayes_test_prediction, average='macro')
print(f"Testing Set: Naive Bayes F1-Score with selected features: {test_f1_score:.2f}\n")

# Evaluate accuracy for training set and testing set
# train_accuracy = accuracy_score(y_train, naive_bayes_train_prediction)
# print(f"Traing Set: Naive Bayes Training Accuracy with selected features: {train_accuracy:.2f}\n")
test_accuracy = accuracy_score(y_test, naive_bayes_test_prediction)
print(f"Testing Set: Naive Bayes Testing Accuracy with selected features: {test_accuracy:.2f}\n")

print("=" * 25)


# Evaluate with precision for training set and testing set
# train_precision = precision_score(y_train, naive_bayes_train_prediction, average='macro')
# print(f"Naive Bayes Precision Score with selected features: {train_precision:.2f}\n")
test_precision = precision_score(y_test, naive_bayes_test_prediction, average='macro')
print(f"Naive Bayes Precision Score with selected features: {test_precision:.2f}\n")

print("=" * 25)

# Evaluate with Recall for training set and testing set
# train_recall = recall_score(y_train, naive_bayes_train_prediction, average='macro')
# print(f"Naive Bayes  Recall Score with selected features: {train_recall:.2f}")
test_recall = recall_score(y_test, naive_bayes_test_prediction, average='macro')
print(f"Naive Bayes Recall Score with selected features: {test_recall:.2f}")

# F1-score for testing set
test_f1_score = f1_score(y_test, naive_bayes_test_prediction, average='macro')
print(f"Testing Set: Naive Bayes F1-Score with selected features: {test_f1_score:.2f}\n")

# Evaluate accuracy for testing set
test_accuracy = accuracy_score(y_test, naive_bayes_test_prediction)
print(f"Testing Set: Naive Bayes Testing Accuracy with selected features: {test_accuracy:.2f}\n")

print("=" * 25)

# Evaluate with precision for testing set
test_precision = precision_score(y_test, naive_bayes_test_prediction, average='macro')
print(f"Naive Bayes Precision Score with selected features: {test_precision:.2f}\n")

print("=" * 25)

# Evaluate with Recall for testing set
test_recall = recall_score(y_test, naive_bayes_test_prediction, average='macro')
print(f"Naive Bayes Recall Score with selected features: {test_recall:.2f}")

"""# Plotting"""

import matplotlib.pyplot as plt
import numpy as np

# Set style for better visualization
plt.style.use('default')

# 1. Model Performance Comparison
models = ['Random Forest', 'KNN', 'Naive Bayes']
metrics = {
    'Accuracy': [
        accuracy_score(y_test, random_forest_test_prediction),
        accuracy_score(y_test, knn_model_test_prediction),
        accuracy_score(y_test, naive_bayes_test_prediction)
    ],
    'F1-Score': [
        f1_score(y_test, random_forest_test_prediction, average='macro'),
        f1_score(y_test, knn_model_test_prediction, average='macro'),
        f1_score(y_test, naive_bayes_test_prediction, average='macro')
    ],
    'Precision': [
        precision_score(y_test, random_forest_test_prediction, average='macro'),
        precision_score(y_test, knn_model_test_prediction, average='macro'),
        precision_score(y_test, naive_bayes_test_prediction, average='macro')
    ],
    'Recall': [
        recall_score(y_test, random_forest_test_prediction, average='macro'),
        recall_score(y_test, knn_model_test_prediction, average='macro'),
        recall_score(y_test, naive_bayes_test_prediction, average='macro')
    ]
}

# Define colors for each metric
colors = ['#CD5D7D', '#F6ECF0', '#A7C5EB', '#949CDF']  # Add colors as needed for each metric

# Create subplots for better organization
plt.figure(figsize=(15, 10))

# Plot each metric
x = np.arange(len(models))
width = 0.2
multiplier = 0

for idx, (metric, scores) in enumerate(metrics.items()):
    offset = width * multiplier
    plt.bar(x + offset, scores, width, label=metric, color=colors[idx])  # Use the corresponding color
    multiplier += 1

plt.xlabel('Models', fontsize=12)
plt.ylabel('Score', fontsize=12)
plt.title('Model Performance Comparison', fontsize=14, pad=20)
plt.xticks(x + width * 1.5, models, fontsize=10)
plt.legend(loc='upper right', fontsize=10)
plt.ylim(0, 1)

# Add value labels on the bars
for i in x:
    for j in range(len(metrics)):
        plt.text(i + width * j, list(metrics.values())[j][i],
                f'{list(metrics.values())[j][i]:.2f}',
                ha='center', va='bottom')

plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()


# 2. Feature Importance Plot for Random Forest
plt.figure(figsize=(12, 6))
feature_importance = pd.DataFrame({
    'feature': X_train.columns[list(sfs_rf.k_feature_idx_)],
    'importance': random_forest.feature_importances_
})
feature_importance = feature_importance.sort_values('importance', ascending=False)

plt.barh(feature_importance['feature'], feature_importance['importance'])
plt.title('Feature Importance in Random Forest Model', fontsize=14, pad=20)
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# 3. Prediction Distribution Plot
plt.figure(figsize=(12, 6))
prediction_comparison = pd.DataFrame({
    'Actual': y_test,
    'Random Forest': random_forest_test_prediction,
    'KNN': knn_model_test_prediction,
    'Naive Bayes': naive_bayes_test_prediction
})

for idx, model in enumerate(['Random Forest', 'KNN', 'Naive Bayes']):
    plt.subplot(1, 3, idx+1)
    confusion = confusion_matrix(prediction_comparison['Actual'],
                               prediction_comparison[model])
    plt.imshow(confusion, cmap='Blues')
    plt.title(f'{model}\nConfusion Matrix', pad=20)
    plt.colorbar()
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    # Add text annotations
    for i in range(confusion.shape[0]):
        for j in range(confusion.shape[1]):
            plt.text(j, i, confusion[i, j],
                    ha='center', va='center')

plt.tight_layout()
plt.show()